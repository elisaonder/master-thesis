{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8759b5b-dd71-4496-befa-0b05744e973a",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "Use this table of content to navigate to the respective section.\n",
    "\n",
    "\n",
    "- [0. Executive Summary](#0.-Executive-Summary)\n",
    "- [1. Introduction](#1.-Introduction)\n",
    "- [2. Data Import / Split Dataa](#2.-Data-Import-/-Split-Data)\n",
    "- [3. Transformer](#3.-Transformer)\n",
    "    - [3.1 Baseline Model](#3.1-Baseline-Model)\n",
    "    - [3.2 LoRA Model](#3.2-LoRA-Model)\n",
    "    - [3.3 Full Fine-tuned Model](#3.3-Full-Fine-tuned-Model)\n",
    "- [4. Results](#4.-Results)\n",
    "\n",
    "# 0. Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1923cc-a16b-4b3f-a55c-e62f4c7e6440",
   "metadata": {},
   "source": [
    "This notebook develops and evaluates transformer-based classifiers for detecting affective polarization in Portuguese parliamentary speeches. A manually labelled dataset of 1,499 interventions is split into stratified training, validation, and test sets (70/15/15), with the training split balanced to a 50/50 ratio between polarized and non-polarized interventions. Three modelling stages are implemented: an unfine-tuned XLM-RoBERTa baseline, parameter-efficient LoRA adapters on top of several pretrained transformers, and full end-to-end fine-tuning of the same architectures. The baseline model performs poorly and essentially fails to distinguish classes, confirming the need for supervised training. LoRA fine-tuning substantially improves performance while updating less than 1% of parameters, with XLM-RoBERTa reaching around 0.68 macro-F1 on the test set. Full fine-tuning further increases accuracy and macro-F1, with the Portuguese BERT model achieving the best results (≈0.79 accuracy, ≈0.78 macro-F1). The notebook concludes that supervised transformers, especially fully fine-tuned Portuguese BERT, provide a strong foundation for measuring affective polarization in ParlaMint-PT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0467c5d2-c314-4fe3-af71-1e01385ab576",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8aaa9-a64f-44f9-adfa-2a471afaeafa",
   "metadata": {},
   "source": [
    "The goal of this notebook is to build a reproducible modelling pipeline for classifying parliamentary interventions as polarized or non-polarized using transformer language models. Starting from a manually annotated sample of Portuguese parliamentary speeches, the workflow first performs data cleaning and construction of stratified train–validation–test splits, balancing the training data to mitigate class imbalance while preserving the natural label distribution in the evaluation sets. On top of these splits, several transformer architectures are trained and compared under different fine-tuning regimes, ranging from a naive baseline without task-specific training to state-of-the-art supervised setups. Throughout, common evaluation metrics (accuracy, macro-F1 and detailed classification reports) are used to assess model quality and to identify the most suitable configuration for downstream analysis of affective polarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d7e82-d57b-4224-b927-6431b935e0e2",
   "metadata": {},
   "source": [
    "# 2. Data Import / Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473cc3d-e43c-4956-aae3-e066ca521721",
   "metadata": {},
   "source": [
    "This section loads the manually labelled intervention dataset and prepares the train/validation/test splits for all later experiments. After reading the Excel/CSV file, it keeps the key metadata columns, drops rows without text or label, and casts the polarization label to a binary integer (0 = non-polarized, 1 = polarized). Using a fixed random seed, it then performs a stratified 70/15/15 split so that the overall class distribution is preserved in the validation and test sets, while the training set is additionally balanced to a 50/50 class ratio by downsampling the majority class. Finally, the three splits are saved to disk and a short summary of the class ratios in each split is printed for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2892c636-8bc3-4db6-a76f-a50d5c4aaf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL CLASS RATIO\n",
      "ALL : n=1499 | 0=901 (60.11%) | 1=598 (39.89%)\n",
      "\n",
      "SPLIT SUMMARY\n",
      "TRAIN (balanced): n=836 | 0=418 (50.0%) | 1=418 (50.0%)\n",
      "VAL           : n=225 | 0=135 (60.0%) | 1=90 (40.0%)\n",
      "TEST          : n=225 | 0=135 (60.0%) | 1=90 (40.0%)\n",
      "\n",
      " Files saved in: C:\\Users\\Nomis\\Desktop\\splits_balanced_train_70_15_15\n"
     ]
    }
   ],
   "source": [
    "# Balance Training Data / Real Distribution for Val & Test\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# config\n",
    "INPUT_PATH = r\"intervention_sample_for_manual_labeling_final.xlsx\"  # or .csv\n",
    "TEXT_COL   = \"text\"\n",
    "LABEL_COL  = \"intervention_polarized_label\"  # 0/1\n",
    "KEEP_COLS  = [\n",
    "    \"speech_id\",\"intervention_id\",\"party\",\"speaker_name\",\n",
    "    \"text\",\"text_length\",\"intervention_polarized_label\",\"intervention_label\"\n",
    "]\n",
    "OUT_DIR = \"splits_balanced_train_70_15_15\"\n",
    "SEED = 42\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Load\n",
    "ext = os.path.splitext(INPUT_PATH)[1].lower()\n",
    "if ext == \".xlsx\":\n",
    "    df = pd.read_excel(INPUT_PATH)\n",
    "elif ext == \".csv\":\n",
    "    df = pd.read_csv(INPUT_PATH)\n",
    "else:\n",
    "    raise ValueError(\"Use .xlsx or .csv\")\n",
    "\n",
    "# 2) Select cols, clean\n",
    "for c in KEEP_COLS:\n",
    "    if c not in df.columns:\n",
    "        df[c] = pd.NA\n",
    "df = df[KEEP_COLS].dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
    "\n",
    "# ensure binary ints\n",
    "df[LABEL_COL] = df[LABEL_COL].astype(int)\n",
    "\n",
    "def show_counts(name, d):\n",
    "    counts = d[LABEL_COL].value_counts().sort_index()\n",
    "    n = len(d)\n",
    "    pcts = (counts / n * 100).round(2)\n",
    "    print(f\"{name}: n={n} | 0={counts.get(0,0)} ({pcts.get(0,0.0)}%) | 1={counts.get(1,0)} ({pcts.get(1,0.0)}%)\")\n",
    "\n",
    "print(\"GLOBAL CLASS RATIO\")\n",
    "show_counts(\"ALL \", df)\n",
    "\n",
    "# 3) Stratified split: 70% train_base, 15% val, 15% test\n",
    "train_base, temp = train_test_split(\n",
    "    df, test_size=0.30, stratify=df[LABEL_COL], random_state=SEED\n",
    ")\n",
    "val, test = train_test_split(\n",
    "    temp, test_size=0.50, stratify=temp[LABEL_COL], random_state=SEED\n",
    ")\n",
    "\n",
    "# 4) Balance TRAIN to 50/50 by downsampling the majority\n",
    "g0 = train_base[train_base[LABEL_COL] == 0]\n",
    "g1 = train_base[train_base[LABEL_COL] == 1]\n",
    "if len(g0) == 0 or len(g1) == 0:\n",
    "    raise RuntimeError(\"Cannot balance train: one class has 0 samples.\")\n",
    "\n",
    "minority_n = min(len(g0), len(g1))\n",
    "g0_bal = g0.sample(n=minority_n, random_state=SEED) if len(g0) > minority_n else g0\n",
    "g1_bal = g1.sample(n=minority_n, random_state=SEED) if len(g1) > minority_n else g1\n",
    "train = pd.concat([g0_bal, g1_bal], axis=0).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# 5) Save\n",
    "train.to_csv(os.path.join(OUT_DIR, \"train.csv\"), index=False)\n",
    "val.to_csv(os.path.join(OUT_DIR, \"val.csv\"), index=False)\n",
    "test.to_csv(os.path.join(OUT_DIR, \"test.csv\"), index=False)\n",
    "\n",
    "print(\"\\nSPLIT SUMMARY\")\n",
    "show_counts(\"TRAIN (balanced)\", train)\n",
    "show_counts(\"VAL           \", val)   # keeps real ratio\n",
    "show_counts(\"TEST          \", test)  # keeps real ratio\n",
    "print(f\"\\n Files saved in: {os.path.abspath(OUT_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a646112-3fe6-4a1b-aceb-753abfd6f512",
   "metadata": {},
   "source": [
    "Overall, the labelled dataset contains 1,499 interventions, with a moderately imbalanced class ratio of about 60% non-polarized (0) and 40% polarized (1). After splitting, the training set was explicitly balanced to 836 examples (418 per class), while the validation and test sets each contain 225 examples and preserve the original distribution (roughly 60% non-polarized, 40% polarized). All three splits are stored in the directory C:\\Users\\Nomis\\Desktop\\splits_balanced_train_70_15_15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99027c-e140-442d-a499-df260ee25585",
   "metadata": {},
   "source": [
    "# 3. Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1469abb-2076-467a-9834-17e335a6ea21",
   "metadata": {},
   "source": [
    "This chapter presents the transformer-based models used for affective polarization classification. Section 3.1 introduces a simple baseline where XLM-RoBERTa is applied without any task-specific fine-tuning, providing a lower bound on performance. Section 3.2 then adds parameter-efficient LoRA adapters on top of three pretrained transformers, and Section 3.3 describes full end-to-end fine-tuning of the same architectures. Together, these experiments make it possible to compare different training strategies and quantify their impact on classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32f466b-3827-45d8-852f-2bc875a1ecdc",
   "metadata": {},
   "source": [
    "## 3.1 Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f191ac-8764-4020-b15f-c4b83a5f6eeb",
   "metadata": {},
   "source": [
    "As an initial baseline, an off-the-shelf XLM-RoBERTa model (xlm-roberta-base) is evaluated on the test split without any task-specific fine-tuning. The script loads the labeled test interventions, tokenizes the text up to 256 tokens, and feeds the resulting sequences through a randomly initialized binary classification head on top of the pretrained encoder. The predicted labels are then compared to the gold labels using accuracy, macro-F1, the confusion matrix, and a detailed classification report, providing a naive reference point against which subsequent fine-tuned models can be assessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b0ec06-0631-4a9d-9c84-68ae8d466195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['speech_id', 'intervention_id', 'party', 'speaker_name', 'text', 'text_length', 'intervention_polarized_label', 'intervention_label']\n",
      "n_test : 225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XLM-R BASELINE (no fine-tuning) on TEST ===\n",
      "Accuracy : 0.4000\n",
      "Macro F1 : 0.2857\n",
      "\n",
      "Confusion matrix [rows=true, cols=pred]:\n",
      "[[  0 135]\n",
      " [  0  90]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       135\n",
      "           1     0.4000    1.0000    0.5714        90\n",
      "\n",
      "    accuracy                         0.4000       225\n",
      "   macro avg     0.2000    0.5000    0.2857       225\n",
      "weighted avg     0.1600    0.4000    0.2286       225\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nomis\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Nomis\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Nomis\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "#CONFIG\n",
    "OUT_DIR   = r\"C:\\Users\\Nomis\\Desktop\\splits_balanced_train_70_15_15\"\n",
    "TEST_CSV  = os.path.join(OUT_DIR, \"test.csv\")\n",
    "\n",
    "TEXT_COL  = \"text\"\n",
    "LABEL_COL = \"intervention_polarized_label\"   # 0/1\n",
    "\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "MAX_LEN    = 256\n",
    "\n",
    "#LOAD TEST DATA\n",
    "df_test = pd.read_csv(TEST_CSV)\n",
    "print(\"Columns:\", list(df_test.columns))\n",
    "print(\"n_test :\", len(df_test))\n",
    "\n",
    "texts  = df_test[TEXT_COL].astype(str).tolist()\n",
    "labels = df_test[LABEL_COL].astype(int).tolist()\n",
    "\n",
    "#LOAD BASELINE XLM-R (NO FINE-TUNING)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#TOKENIZE & RUN\n",
    "encodings = tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=MAX_LEN,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids      = encodings[\"input_ids\"].to(device)\n",
    "attention_mask = encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "#METRICS\n",
    "acc = accuracy_score(labels, preds)\n",
    "f1  = f1_score(labels, preds, average=\"macro\")\n",
    "cm  = confusion_matrix(labels, preds)\n",
    "\n",
    "print(\"\\n=== XLM-R BASELINE (no fine-tuning) on TEST ===\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Macro F1 : {f1:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix [rows=true, cols=pred]:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(labels, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7267f5-087b-40a6-8498-54198c44c981",
   "metadata": {},
   "source": [
    "The unfine-tuned XLM-RoBERTa baseline performs poorly on the polarization task. On the 225 test interventions, it achieves an accuracy of 0.40 and a macro-F1 of 0.29, predicting exclusively the polarized class (1). This is reflected in the confusion matrix, where no non-polarized examples (0) are correctly identified, confirming that a randomly initialized classification head without task-specific training is not suitable for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845976b1-c304-400b-9511-63f83fb97087",
   "metadata": {},
   "source": [
    "## 3.2 LoRA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2101351c-a919-4397-9c50-e0776e4df322",
   "metadata": {},
   "source": [
    "This script implements the supervised fine-tuning pipeline for Portuguese parliamentary sentiment (affective polarization) classification. Starting from the manually labeled interventions, it performs a stratified 70/15/15 train–validation–test split and balances only the training set to a 50/50 class ratio, while preserving the natural label distribution in validation and test. The data is converted into Hugging Face Dataset objects, tokenized with a maximum sequence length of 256 tokens, and then used to fine-tune several transformer architectures (neuralmind/bert-base-portuguese-cased, xlm-roberta-base, and bert-base-multilingual-cased) under a common training configuration (5 epochs, learning rate 2e-5, weight decay, warmup, batch sizes). Training is carried out via the Trainer API, with accuracy and macro/weighted precision, recall, and F1 computed for both validation and test sets, complemented by a detailed classification report on the test set. For each model, the fine-tuned weights, tokenizer, and evaluation metrics are saved to disk, and a consolidated summary table is written to allow direct comparison of model performance and training cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5cac624-204a-4d46-8584-c3932a66b091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full (original) label counts:\n",
      " intervention_polarized_label\n",
      "0    901\n",
      "1    598\n",
      "Name: count, dtype: int64\n",
      "Full (original) label %:\n",
      " intervention_polarized_label\n",
      "0    60.107\n",
      "1    39.893\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Split sizes:\n",
      "Train: 1049, Val: 225, Test: 225\n",
      "\n",
      "TRAIN balanced counts:\n",
      " intervention_polarized_label\n",
      "1    418\n",
      "0    418\n",
      "Name: count, dtype: int64\n",
      "VALID counts:\n",
      " intervention_polarized_label\n",
      "0    135\n",
      "1     90\n",
      "Name: count, dtype: int64\n",
      "TEST counts:\n",
      " intervention_polarized_label\n",
      "0    135\n",
      "1     90\n",
      "Name: count, dtype: int64\n",
      "Using device: cuda\n",
      "\n",
      "===== neuralmind/bert-base-portuguese-cased (LoRA) =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcfbed581314c8d82d0ca61b581ffb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594a0e7770c84ff39580b6bb10fff29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af83039af34f4992a837b716810d4435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Nomis\\AppData\\Local\\Temp\\ipykernel_260\\3074950906.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 443,906 / 109,368,580 (0.41%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='265' max='265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [265/265 02:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.678800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.680300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.670300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8393    0.3481    0.4921       135\n",
      "           1     0.4793    0.9000    0.6255        90\n",
      "\n",
      "    accuracy                         0.5689       225\n",
      "   macro avg     0.6593    0.6241    0.5588       225\n",
      "weighted avg     0.6953    0.5689    0.5455       225\n",
      "\n",
      "Saved LoRA adapter + tokenizer to: ./pt_sentiment_models_3split_lora\\neuralmind_bert-base-portuguese-cased\n",
      "Time: 151.5s\n",
      "\n",
      "===== xlm-roberta-base (LoRA) =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d178687e85427fafe24eef20455765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4573f7f21244d18a87eed4dfcec3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39adab7c881426b9ba75a02432fc951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Nomis\\AppData\\Local\\Temp\\ipykernel_260\\3074950906.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,034,498 / 279,079,684 (0.37%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='265' max='265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [265/265 02:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.662100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.659900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8539    0.5630    0.6786       135\n",
      "           1     0.5662    0.8556    0.6814        90\n",
      "\n",
      "    accuracy                         0.6800       225\n",
      "   macro avg     0.7101    0.7093    0.6800       225\n",
      "weighted avg     0.7388    0.6800    0.6797       225\n",
      "\n",
      "Saved LoRA adapter + tokenizer to: ./pt_sentiment_models_3split_lora\\xlm-roberta-base\n",
      "Time: 154.7s\n",
      "\n",
      "===== bert-base-multilingual-cased (LoRA) =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51e70073c304a2e88feb63be5f8dd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd55d397602b47aa9d850412a2112efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832e7df2641f4644bec9a21d4a475e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Nomis\\AppData\\Local\\Temp\\ipykernel_260\\3074950906.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 443,906 / 178,298,884 (0.25%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='265' max='265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [265/265 02:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.689500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.685700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.679400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7765    0.4889    0.6000       135\n",
      "           1     0.5071    0.7889    0.6174        90\n",
      "\n",
      "    accuracy                         0.6089       225\n",
      "   macro avg     0.6418    0.6389    0.6087       225\n",
      "weighted avg     0.6687    0.6089    0.6070       225\n",
      "\n",
      "Saved LoRA adapter + tokenizer to: ./pt_sentiment_models_3split_lora\\bert-base-multilingual-cased\n",
      "Time: 154.3s\n",
      "\n",
      "==== Summary (70/15/15 split — LoRA) ====\n",
      "                                model mode  epochs  val_accuracy  val_f1_macro  test_accuracy  test_f1_macro  params_trainable  time_sec                                                                saved_to\n",
      "neuralmind/bert-base-portuguese-cased lora       5      0.622222      0.618576       0.568889       0.558815            443906     151.5 ./pt_sentiment_models_3split_lora\\neuralmind_bert-base-portuguese-cased\n",
      "                     xlm-roberta-base lora       5      0.662222      0.662162       0.680000       0.679994           1034498     154.7                      ./pt_sentiment_models_3split_lora\\xlm-roberta-base\n",
      "         bert-base-multilingual-cased lora       5      0.613333      0.613211       0.608889       0.608696            443906     154.3          ./pt_sentiment_models_3split_lora\\bert-base-multilingual-cased\n",
      "\n",
      "Saved all outputs to: ./pt_sentiment_models_3split_lora\n"
     ]
    }
   ],
   "source": [
    "# LoRA Model — 70/15/15 split \n",
    "# Train: balanced (50/50), Val/Test: real distribution\n",
    "import os, time, warnings, numpy as np, pandas as pd, torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, TrainingArguments, Trainer\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n",
    "\n",
    "# paths & columns\n",
    "DATA_PATH  = r\"C:\\Users\\Nomis\\Downloads\\intervention_sample_for_manual_labeling_new.xlsx\"\n",
    "TEXT_COL   = \"text\"\n",
    "LABEL_COL  = \"intervention_polarized_label\"\n",
    "\n",
    "# training config\n",
    "SEED       = 42\n",
    "EPOCHS     = 5\n",
    "LR         = 2e-5\n",
    "BSZ_TRAIN  = 16\n",
    "BSZ_EVAL   = 32\n",
    "MAX_LEN    = 256\n",
    "WARMUP     = 0.06\n",
    "WEIGHT_DEC = 0.01\n",
    "OUT_DIR    = \"./pt_sentiment_models_3split_lora\"\n",
    "\n",
    "# LoRA hyperparams\n",
    "LORA_R         = 8\n",
    "LORA_ALPHA     = 16\n",
    "LORA_DROPOUT   = 0.1\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"neuralmind/bert-base-portuguese-cased\",\n",
    "    \"xlm-roberta-base\",\n",
    "    \"bert-base-multilingual-cased\",\n",
    "]\n",
    "\n",
    "# load data\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "df = df.dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
    "df[LABEL_COL] = df[LABEL_COL].astype(int)\n",
    "\n",
    "print(\"Full (original) label counts:\\n\", df[LABEL_COL].value_counts())\n",
    "print(\"Full (original) label %:\\n\", (df[LABEL_COL].value_counts(normalize=True) * 100).round(3))\n",
    "\n",
    "# 70/15/15 (stratified)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.3, stratify=df[LABEL_COL], random_state=SEED\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df[LABEL_COL], random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# balance only TRAIN (50/50)\n",
    "n0 = (train_df[LABEL_COL] == 0).sum()\n",
    "n1 = (train_df[LABEL_COL] == 1).sum()\n",
    "n_min = min(n0, n1)\n",
    "\n",
    "train_balanced = pd.concat([\n",
    "    train_df[train_df[LABEL_COL] == 0].sample(n_min, random_state=SEED),\n",
    "    train_df[train_df[LABEL_COL] == 1].sample(n_min, random_state=SEED),\n",
    "]).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTRAIN balanced counts:\\n\", train_balanced[LABEL_COL].value_counts())\n",
    "print(\"VALID counts:\\n\", val_df[LABEL_COL].value_counts())\n",
    "print(\"TEST counts:\\n\", test_df[LABEL_COL].value_counts())\n",
    "\n",
    "# to HF datasets\n",
    "train_ds = Dataset.from_pandas(train_balanced[[TEXT_COL, LABEL_COL]].reset_index(drop=True))\n",
    "val_ds   = Dataset.from_pandas(val_df[[TEXT_COL, LABEL_COL]].reset_index(drop=True))\n",
    "test_ds  = Dataset.from_pandas(test_df[[TEXT_COL, LABEL_COL]].reset_index(drop=True))\n",
    "\n",
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# helpers\n",
    "def build_tokenizer(model_name):\n",
    "    return AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "def tokenize_fn(batch, tok):\n",
    "    return tok(batch[TEXT_COL], truncation=True, padding=False, max_length=MAX_LEN)\n",
    "\n",
    "def _rename_to_labels(ds):\n",
    "    if LABEL_COL in ds.column_names:\n",
    "        ds = ds.rename_column(LABEL_COL, \"labels\")\n",
    "    return ds\n",
    "\n",
    "def _set_torch_format(ds):\n",
    "    cols = [c for c in [\"input_ids\",\"attention_mask\",\"token_type_ids\",\"labels\"] if c in ds.column_names]\n",
    "    ds.set_format(\"torch\", columns=cols)\n",
    "    return ds\n",
    "\n",
    "def prepare_datasets(tok):\n",
    "    tds = train_ds.map(lambda b: tokenize_fn(b, tok), batched=True)\n",
    "    vds = val_ds.map(lambda b: tokenize_fn(b, tok), batched=True)\n",
    "    tts = test_ds.map(lambda b: tokenize_fn(b, tok), batched=True)\n",
    "    tds = _rename_to_labels(tds); vds = _rename_to_labels(vds); tts = _rename_to_labels(tts)\n",
    "    tds = _set_torch_format(tds); vds = _set_torch_format(vds); tts = _set_torch_format(tts)\n",
    "    return tds, vds, tts\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    pM, rM, f1M, _  = precision_recall_fscore_support(labels, preds, average=\"macro\",    zero_division=0)\n",
    "    pW, rW, f1W, _  = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    return {\n",
    "        \"accuracy\":acc, \"precision_macro\":pM, \"recall_macro\":rM, \"f1_macro\":f1M,\n",
    "        \"precision_weighted\":pW, \"recall_weighted\":rW, \"f1_weighted\":f1W\n",
    "    }\n",
    "\n",
    "def make_args(out_dir):\n",
    "    return TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        per_device_train_batch_size=BSZ_TRAIN,\n",
    "        per_device_eval_batch_size=BSZ_EVAL,\n",
    "        learning_rate=LR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        warmup_ratio=WARMUP,\n",
    "        weight_decay=WEIGHT_DEC,\n",
    "        logging_steps=50,\n",
    "        seed=SEED,\n",
    "        fp16=(device==\"cuda\"),\n",
    "        dataloader_num_workers=0\n",
    "    )\n",
    "\n",
    "def target_modules_for(model):\n",
    "    names = [n for n, _ in model.named_parameters()]\n",
    "    # XLM-R / RoBERTa-style use q_proj/k_proj/v_proj; BERT-style often \"query/key/value\"\n",
    "    if any(\".q_proj.\" in n for n in names) or any(\".k_proj.\" in n for n in names):\n",
    "        return [\"q_proj\",\"k_proj\",\"v_proj\"]\n",
    "    return [\"query\",\"key\",\"value\"]\n",
    "\n",
    "def train_and_eval(model_name):\n",
    "    print(f\"\\n===== {model_name} (LoRA) =====\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    tok = build_tokenizer(model_name)\n",
    "    tds, vds, tts = prepare_datasets(tok)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2, problem_type=\"single_label_classification\"\n",
    "    )\n",
    "\n",
    "    lcfg = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=target_modules_for(base)\n",
    "    )\n",
    "    model = get_peft_model(base, lcfg)\n",
    "\n",
    "    # how many params train\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total     = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "\n",
    "    model_out_dir = os.path.join(OUT_DIR, model_name.replace(\"/\", \"_\"))\n",
    "    args = make_args(model_out_dir)\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args, train_dataset=tds, eval_dataset=vds,\n",
    "        tokenizer=tok, data_collator=collator, compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics_val  = trainer.evaluate(eval_dataset=vds)\n",
    "    metrics_test = trainer.evaluate(eval_dataset=tts)\n",
    "\n",
    "    # Detailed report on TEST\n",
    "    pred_out = trainer.predict(tts)\n",
    "    preds  = np.argmax(pred_out.predictions, axis=-1)\n",
    "    labels = np.array(tts[\"labels\"])\n",
    "    report = classification_report(labels, preds, target_names=[\"0\",\"1\"], digits=4, zero_division=0)\n",
    "    print(\"Test-set report:\\n\", report)\n",
    "\n",
    "    # ---- save LoRA adapter + tokenizer ----\n",
    "    # This saves only PEFT adapter weights (small) into model_out_dir\n",
    "    trainer.save_model(model_out_dir)     # adapter_config + adapter_model\n",
    "    tok.save_pretrained(model_out_dir)\n",
    "    # also store a small README with reload snippet\n",
    "    with open(os.path.join(model_out_dir, \"LOAD_ADAPTER.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            \"Reload example:\\n\"\n",
    "            \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\"\n",
    "            \"from peft import PeftModel\\n\\n\"\n",
    "            f'base = AutoModelForSequenceClassification.from_pretrained(\"{model_name}\", num_labels=2)\\n'\n",
    "            f'tok  = AutoTokenizer.from_pretrained(r\"{model_out_dir}\")\\n'\n",
    "            f'model= PeftModel.from_pretrained(base, r\"{model_out_dir}\")\\n'\n",
    "            \"model.eval()\\n\"\n",
    "        )\n",
    "\n",
    "    print(f\"Saved LoRA adapter + tokenizer to: {model_out_dir}\")\n",
    "    print(f\"Time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"mode\": \"lora\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"val_accuracy\": metrics_val.get(\"eval_accuracy\", np.nan),\n",
    "        \"val_f1_macro\": metrics_val.get(\"eval_f1_macro\", np.nan),\n",
    "        \"test_accuracy\": metrics_test.get(\"eval_accuracy\", np.nan),\n",
    "        \"test_f1_macro\": metrics_test.get(\"eval_f1_macro\", np.nan),\n",
    "        \"params_trainable\": int(trainable),\n",
    "        \"time_sec\": round(time.time()-t0, 1),\n",
    "        \"saved_to\": model_out_dir\n",
    "    }, report\n",
    "\n",
    "# run\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "rows, reports = [], {}\n",
    "\n",
    "for m in MODEL_NAMES:\n",
    "    try:\n",
    "        row, rep = train_and_eval(m)\n",
    "        rows.append(row); reports[m] = rep\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {m}: {e}\")\n",
    "        rows.append({\"model\": m, \"error\": str(e)})\n",
    "\n",
    "summary = pd.DataFrame(rows)\n",
    "print(\"\\n==== Summary (70/15/15 split — LoRA) ====\")\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "summary.to_csv(os.path.join(OUT_DIR, \"model_comparison_3split_lora.csv\"), index=False)\n",
    "with open(os.path.join(OUT_DIR, \"per_class_reports_3split_lora.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for m, r in reports.items():\n",
    "        f.write(f\"===== {m} =====\\n{r}\\n\\n\")\n",
    "\n",
    "print(f\"\\nSaved all outputs to: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f3960-0589-4e71-85e7-74081e28bc56",
   "metadata": {},
   "source": [
    "The full labelled dataset comprises 1,499 interventions, with 60% non-polarized and 40% polarized speeches. It is split into 1,049 training, 225 validation, and 225 test examples; the training set is subsequently balanced to 418 instances per class, while validation and test retain the natural class distribution (135 non-polarized, 90 polarized). All experiments are run on GPU (cuda) using LoRA fine-tuning, updating only 0.25–0.41% of the model parameters. Among the three architectures, XLM-RoBERTa with LoRA achieves the strongest results with a test accuracy of 0.68 and a macro-F1 of 0.68, while the Portuguese BERT and multilingual BERT baselines reach macro-F1 scores of 0.56 and 0.61, respectively, indicating that LoRA adapters on top of XLM-RoBERTa offer the best trade-off between performance and training efficiency for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103f3f0b-9e04-462b-ac6d-be505fe7fed4",
   "metadata": {},
   "source": [
    "## 3.3 Full Fine-tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd6f77-dddf-41d4-b7d6-a78633daf9ad",
   "metadata": {},
   "source": [
    "This script implements the main supervised fine-tuning experiments on the full transformer models. Starting from the 1,499 manually labelled interventions, it performs a stratified 70/15/15 split and balances only the training set to a 50/50 ratio between polarized and non-polarized interventions, while validation and test retain the natural class distribution. The text is converted into Hugging Face Dataset objects, tokenized with a maximum length of 256 tokens, and used to fully fine-tune three architectures—neuralmind/bert-base-portuguese-cased, xlm-roberta-base, and bert-base-multilingual-cased—with a shared configuration of 5 epochs, learning rate 2×10⁻⁵, weight decay, warmup, and batch sizes of 16/32. In contrast to the LoRA setting, all model parameters (encoder and classification head) are updated. Training is carried out via the Trainer API, computing accuracy and macro/weighted precision, recall, and F1 on the validation and test sets, and generating a detailed classification report on the test data. For each model, the fine-tuned weights, tokenizer, and evaluation metrics are saved to disk, and a summary table is exported to compare performance and training cost across architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6abaf311-cfdd-42f5-8ed0-b61fdfee6990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full (original) label counts:\n",
      " intervention_polarized_label\n",
      "0    901\n",
      "1    598\n",
      "Name: count, dtype: int64\n",
      "Full (original) label %:\n",
      " intervention_polarized_label\n",
      "0    60.107\n",
      "1    39.893\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Split sizes:\n",
      "Train: 1049, Val: 225, Test: 225\n",
      "\n",
      "TRAIN balanced counts:\n",
      " intervention_polarized_label\n",
      "1    418\n",
      "0    418\n",
      "Name: count, dtype: int64\n",
      "VALID counts:\n",
      " intervention_polarized_label\n",
      "0    135\n",
      "1     90\n",
      "Name: count, dtype: int64\n",
      "TEST counts:\n",
      " intervention_polarized_label\n",
      "0    135\n",
      "1     90\n",
      "Name: count, dtype: int64\n",
      "Using device: cuda\n",
      "\n",
      "===== neuralmind/bert-base-portuguese-cased (full) =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dad2f3a4d3643609ef49825c9af454a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c4c5268d944e2a9ffd20437da3cec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbec732c6414beba1f95409b4790c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Nomis\\AppData\\Local\\Temp\\ipykernel_260\\381365720.py:179: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 108,924,674 / 108,924,674 (100.00%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='265' max='265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [265/265 03:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.474100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.373400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.270100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.211600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test-set report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8595    0.7704    0.8125       135\n",
      "           1     0.7019    0.8111    0.7526        90\n",
      "\n",
      "    accuracy                         0.7867       225\n",
      "   macro avg     0.7807    0.7907    0.7825       225\n",
      "weighted avg     0.7965    0.7867    0.7885       225\n",
      "\n",
      "Saved trained model + tokenizer → .\\pt_sentiment_models_3split_new\\neuralmind_bert-base-portuguese-cased\\final_model\n",
      "Time: 199.0s\n",
      "\n",
      "===== xlm-roberta-base (full) =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c716b8e89c4a7c881d57105eee7531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5145d60ad35642c59fad9aab0ff7cc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fecb8dd9304088a5bd97f864c0729e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Nomis\\AppData\\Local\\Temp\\ipykernel_260\\381365720.py:179: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 278,045,186 / 278,045,186 (100.00%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='265' max='265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [265/265 05:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.658200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.573000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.522600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.412100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test-set report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8942    0.6889    0.7782       135\n",
      "           1     0.6529    0.8778    0.7488        90\n",
      "\n",
      "    accuracy                         0.7644       225\n",
      "   macro avg     0.7736    0.7833    0.7635       225\n",
      "weighted avg     0.7977    0.7644    0.7665       225\n",
      "\n",
      "Saved trained model + tokenizer → .\\pt_sentiment_models_3split_new\\xlm-roberta-base\\final_model\n",
      "Time: 352.0s\n",
      "\n",
      "===== bert-base-multilingual-cased (full) =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b8f8769d344eceac05ebc1ad4bea4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739cec19917a49b59d4448fea9e04754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b981ec38de04871924b4f123fb30474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Nomis\\AppData\\Local\\Temp\\ipykernel_260\\381365720.py:179: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 177,854,978 / 177,854,978 (100.00%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='265' max='265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [265/265 03:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.633200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.522800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.351200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test-set report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8482    0.7037    0.7692       135\n",
      "           1     0.6460    0.8111    0.7192        90\n",
      "\n",
      "    accuracy                         0.7467       225\n",
      "   macro avg     0.7471    0.7574    0.7442       225\n",
      "weighted avg     0.7673    0.7467    0.7492       225\n",
      "\n",
      "Saved trained model + tokenizer → .\\pt_sentiment_models_3split_new\\bert-base-multilingual-cased\\final_model\n",
      "Time: 216.5s\n",
      "\n",
      "==== Summary (70/15/15 split, TRAIN balanced) ====\n",
      "                                model mode  epochs  val_accuracy  val_f1_macro  test_accuracy  test_f1_macro  params_trainable                                                                                                  save_dir  time_sec\n",
      "neuralmind/bert-base-portuguese-cased full       5      0.760000      0.752566       0.786667       0.782539         108924674 C:\\Users\\Nomis\\Downloads\\pt_sentiment_models_3split_new\\neuralmind_bert-base-portuguese-cased\\final_model     199.0\n",
      "                     xlm-roberta-base full       5      0.782222      0.779275       0.764444       0.763529         278045186                      C:\\Users\\Nomis\\Downloads\\pt_sentiment_models_3split_new\\xlm-roberta-base\\final_model     352.0\n",
      "         bert-base-multilingual-cased full       5      0.777778      0.774531       0.746667       0.744221         177854978          C:\\Users\\Nomis\\Downloads\\pt_sentiment_models_3split_new\\bert-base-multilingual-cased\\final_model     216.5\n",
      "\n",
      "All outputs saved under: C:\\Users\\Nomis\\Downloads\\pt_sentiment_models_3split_new\n"
     ]
    }
   ],
   "source": [
    "# Full Fine Tuning — 70/15/15 split\n",
    "import os, json, time, warnings, numpy as np, pandas as pd, torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, TrainingArguments, Trainer\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n",
    "\n",
    "# paths & columns \n",
    "DATA_PATH  = r\"C:\\Users\\Nomis\\Downloads\\intervention_sample_for_manual_labeling_new.xlsx\"  # <-- new file\n",
    "TEXT_COL   = \"text\"\n",
    "LABEL_COL  = \"intervention_polarized_label\"\n",
    "\n",
    "# ---- training config ----\n",
    "SEED       = 42\n",
    "EPOCHS     = 5\n",
    "LR         = 2e-5\n",
    "BSZ_TRAIN  = 16\n",
    "BSZ_EVAL   = 32\n",
    "MAX_LEN    = 256\n",
    "WARMUP     = 0.06\n",
    "WEIGHT_DEC = 0.01\n",
    "OUT_DIR    = r\".\\pt_sentiment_models_3split_new\"  # results root\n",
    "TRAINING_MODE = \"full\"   \n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"neuralmind/bert-base-portuguese-cased\",\n",
    "    \"xlm-roberta-base\",\n",
    "    \"bert-base-multilingual-cased\",\n",
    "]\n",
    "\n",
    "# load & inspect data \n",
    "df = pd.read_excel(DATA_PATH)\n",
    "df = df.dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
    "df[LABEL_COL] = df[LABEL_COL].astype(int)\n",
    "\n",
    "print(\"Full (original) label counts:\\n\", df[LABEL_COL].value_counts())\n",
    "print(\"Full (original) label %:\\n\", (df[LABEL_COL].value_counts(normalize=True) * 100).round(3))\n",
    "\n",
    "# 70/15/15 (stratified)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, stratify=df[LABEL_COL], random_state=SEED)\n",
    "val_df,   test_df = train_test_split(temp_df, test_size=0.50, stratify=temp_df[LABEL_COL], random_state=SEED)\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# balance only TRAIN (50/50)\n",
    "n0, n1   = (train_df[LABEL_COL] == 0).sum(), (train_df[LABEL_COL] == 1).sum()\n",
    "n_min    = min(n0, n1)\n",
    "train_balanced = pd.concat([\n",
    "    train_df[train_df[LABEL_COL] == 0].sample(n_min, random_state=SEED),\n",
    "    train_df[train_df[LABEL_COL] == 1].sample(n_min, random_state=SEED),\n",
    "]).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTRAIN balanced counts:\\n\", train_balanced[LABEL_COL].value_counts())\n",
    "print(\"VALID counts:\\n\", val_df[LABEL_COL].value_counts())\n",
    "print(\"TEST counts:\\n\", test_df[LABEL_COL].value_counts())\n",
    "\n",
    "# to HF datasets\n",
    "train_ds = Dataset.from_pandas(train_balanced[[TEXT_COL, LABEL_COL]].reset_index(drop=True))\n",
    "val_ds   = Dataset.from_pandas(val_df[[TEXT_COL, LABEL_COL]].reset_index(drop=True))\n",
    "test_ds  = Dataset.from_pandas(test_df[[TEXT_COL, LABEL_COL]].reset_index(drop=True))\n",
    "\n",
    "# device\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device_str)\n",
    "\n",
    "# helpers \n",
    "def build_tokenizer(model_name):\n",
    "    return AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "def tokenize_fn(batch, tok):\n",
    "    return tok(batch[TEXT_COL], truncation=True, padding=False, max_length=MAX_LEN)\n",
    "\n",
    "def _rename_to_labels(ds):\n",
    "    if LABEL_COL in ds.column_names:\n",
    "        ds = ds.rename_column(LABEL_COL, \"labels\")\n",
    "    return ds\n",
    "\n",
    "def _set_torch_format(ds):\n",
    "    cols = [c for c in [\"input_ids\",\"attention_mask\",\"token_type_ids\",\"labels\"] if c in ds.column_names]\n",
    "    ds.set_format(\"torch\", columns=cols)\n",
    "    return ds\n",
    "\n",
    "def prepare_datasets(tok):\n",
    "    tds = train_ds.map(lambda b: tokenize_fn(b, tok), batched=True)\n",
    "    vds = val_ds.map(lambda b: tokenize_fn(b, tok), batched=True)\n",
    "    tts = test_ds.map(lambda b: tokenize_fn(b, tok), batched=True)\n",
    "    tds = _rename_to_labels(tds); vds = _rename_to_labels(vds); tts = _rename_to_labels(tts)\n",
    "    tds = _set_torch_format(tds); vds = _set_torch_format(vds); tts = _set_torch_format(tts)\n",
    "    return tds, vds, tts\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    pM, rM, f1M, _  = precision_recall_fscore_support(labels, preds, average=\"macro\",    zero_division=0)\n",
    "    pW, rW, f1W, _  = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_macro\": pM, \"recall_macro\": rM, \"f1_macro\": f1M,\n",
    "        \"precision_weighted\": pW, \"recall_weighted\": rW, \"f1_weighted\": f1W,\n",
    "    }\n",
    "\n",
    "def make_args(out_dir):\n",
    "    return TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        per_device_train_batch_size=BSZ_TRAIN,\n",
    "        per_device_eval_batch_size=BSZ_EVAL,\n",
    "        learning_rate=LR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        warmup_ratio=WARMUP,\n",
    "        weight_decay=WEIGHT_DEC,\n",
    "        logging_steps=50,\n",
    "        seed=SEED,\n",
    "        fp16=(device_str==\"cuda\"),\n",
    "        dataloader_num_workers=0,\n",
    "        save_steps=0,  \n",
    "    )\n",
    "\n",
    "def set_train_mode(model):\n",
    "    if TRAINING_MODE == \"head\":\n",
    "        for name, p in model.named_parameters():\n",
    "            if not name.startswith(\"classifier\"):\n",
    "                p.requires_grad = False\n",
    "    elif TRAINING_MODE == \"full\":\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "    return model\n",
    "\n",
    "def train_eval_save(model_name):\n",
    "    print(f\"\\n===== {model_name} ({TRAINING_MODE}) =====\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    # per-model dirs\n",
    "    run_dir   = os.path.join(OUT_DIR, model_name.replace(\"/\", \"_\"))\n",
    "    save_dir  = os.path.join(run_dir, \"final_model\")\n",
    "    os.makedirs(run_dir, exist_ok=True); os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    tok = build_tokenizer(model_name)\n",
    "    tds, vds, tts = prepare_datasets(tok)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2, problem_type=\"single_label_classification\"\n",
    "    )\n",
    "\n",
    "    if TRAINING_MODE == \"lora\" and peft_available:\n",
    "        names = [n for n,_ in base.named_parameters()]\n",
    "        targets = [\"q_proj\",\"k_proj\",\"v_proj\"] if any(\".q_proj.\" in n for n in names) else [\"query\",\"key\",\"value\"]\n",
    "        from peft import LoraConfig, TaskType, get_peft_model\n",
    "        lcfg = LoraConfig(task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1, target_modules=targets)\n",
    "        model = get_peft_model(base, lcfg)\n",
    "    else:\n",
    "        model = set_train_mode(base)\n",
    "\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total     = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "\n",
    "    args = make_args(run_dir)\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args, train_dataset=tds, eval_dataset=vds,\n",
    "        tokenizer=tok, data_collator=collator, compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on val + test\n",
    "    metrics_val  = trainer.evaluate(eval_dataset=vds)\n",
    "    metrics_test = trainer.evaluate(eval_dataset=tts)\n",
    "\n",
    "    # Detailed classification report (TEST)\n",
    "    pred_out = trainer.predict(tts)\n",
    "    preds  = np.argmax(pred_out.predictions, axis=-1)\n",
    "    labels = np.array(tts[\"labels\"])\n",
    "    report = classification_report(labels, preds, target_names=[\"0\",\"1\"], digits=4, zero_division=0)\n",
    "    print(\"\\nTest-set report:\\n\", report)\n",
    "\n",
    "    # SAVE TRAINED MODEL + TOKENIZER\n",
    "    trainer.save_model(save_dir)           # model weights, config\n",
    "    tok.save_pretrained(save_dir)          # tokenizer files\n",
    "    # also store metrics\n",
    "    with open(os.path.join(run_dir, \"val_metrics.json\"), \"w\") as f: json.dump(metrics_val, f, indent=2)\n",
    "    with open(os.path.join(run_dir, \"test_metrics.json\"), \"w\") as f: json.dump(metrics_test, f, indent=2)\n",
    "    with open(os.path.join(run_dir, \"test_report.txt\"), \"w\", encoding=\"utf-8\") as f: f.write(report)\n",
    "\n",
    "    print(f\"Saved trained model + tokenizer → {save_dir}\")\n",
    "    print(f\"Time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"mode\": TRAINING_MODE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"val_accuracy\": metrics_val.get(\"eval_accuracy\", np.nan),\n",
    "        \"val_f1_macro\": metrics_val.get(\"eval_f1_macro\", np.nan),\n",
    "        \"test_accuracy\": metrics_test.get(\"eval_accuracy\", np.nan),\n",
    "        \"test_f1_macro\": metrics_test.get(\"eval_f1_macro\", np.nan),\n",
    "        \"params_trainable\": int(trainable),\n",
    "        \"save_dir\": os.path.abspath(save_dir),\n",
    "        \"time_sec\": round(time.time()-t0, 1),\n",
    "    }\n",
    "\n",
    "# run all\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "rows = []\n",
    "for m in MODEL_NAMES:\n",
    "    try:\n",
    "        rows.append(train_eval_save(m))\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {m}: {e}\")\n",
    "        rows.append({\"model\": m, \"error\": str(e)})\n",
    "\n",
    "summary = pd.DataFrame(rows)\n",
    "print(\"\\n==== Summary (70/15/15 split, TRAIN balanced) ====\")\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "summary.to_csv(os.path.join(OUT_DIR, \"model_comparison_3split.csv\"), index=False)\n",
    "print(f\"\\nAll outputs saved under: {os.path.abspath(OUT_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac598477-b27d-466f-bdf9-c7be1c2c9070",
   "metadata": {},
   "source": [
    "The full dataset comprises 1,499 labeled interventions, split into 1,049 training, 225 validation, and 225 test examples; the training set is balanced to 418 polarized and 418 non-polarized cases, while validation and test retain the original 60/40 class ratio (135 vs. 90). Under full fine-tuning, all three transformer models achieve substantially better performance than the unfine-tuned baseline and their LoRA counterparts: neuralmind/bert-base-portuguese-cased performs best with a test accuracy of 0.79 and macro-F1 of 0.78, followed by xlm-roberta-base (accuracy 0.76, macro-F1 0.76) and bert-base-multilingual-cased (accuracy 0.75, macro-F1 0.74). All models show balanced performance across both classes, indicating that full fine-tuning on the balanced training split yields robust affective polarization classifiers for Portuguese parliamentary speeches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60920e-ab16-46e3-abf2-f4d30dbede6d",
   "metadata": {},
   "source": [
    "# 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa891926-0c51-4596-ac7d-e8a87dfaab25",
   "metadata": {},
   "source": [
    "The experiments confirm that task-specific fine-tuning is essential for reliable detection of affective polarization. The unfine-tuned XLM-RoBERTa baseline performs poorly, achieving only 0.40 accuracy and 0.29 macro-F1 on the test set and predicting almost exclusively the polarized class. Introducing LoRA adapters already leads to substantial gains: XLM-RoBERTa with LoRA reaches 0.68 accuracy and 0.68 macro-F1, while the Portuguese and multilingual BERT models achieve macro-F1 scores of 0.56 and 0.61, respectively. Full end-to-end fine-tuning further improves performance across all architectures. The best results are obtained with neuralmind/bert-base-portuguese-cased, which attains 0.79 test accuracy and 0.78 macro-F1, followed by xlm-roberta-base (0.76/0.76) and bert-base-multilingual-cased (0.75/0.74). Overall, the results indicate that fully fine-tuned Portuguese BERT provides the most effective and balanced classifier for the binary polarization task on ParlaMint-PT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
